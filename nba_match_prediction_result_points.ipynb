{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "language_info": {
      "name": "python",
      "version": "3.6.6",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "colab": {
      "name": "nba-match-prediction-result-points.ipynb",
      "provenance": [],
      "toc_visible": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xLpajwiZnZnN"
      },
      "source": [
        "This kernel not including EDA because this kernel is too long and our main goal is building prediction model.\n",
        "For features,We just collected required features."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NMQdYzxhnZnQ"
      },
      "source": [
        "# 1-Preprocessing\n",
        "    We do this job by seperating classes or files all code.\n",
        "    Then,We use all these methods for build our model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZK5JIxh-nZnT",
        "outputId": "4303221e-27dc-42c8-9fec-b7e1a9358ebc"
      },
      "source": [
        "!mkdir labelencoder\n",
        "!mkdir onehotencoder\n",
        "!mkdir inputs\n",
        "!mkdir standartScaler\n",
        "!mkdir test_data\n",
        "!mkdir specific\n",
        "!mkdir model\n",
        "!mkdir model/teams"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "mkdir: cannot create directory ‘labelencoder’: File exists\n",
            "mkdir: cannot create directory ‘onehotencoder’: File exists\n",
            "mkdir: cannot create directory ‘inputs’: File exists\n",
            "mkdir: cannot create directory ‘standartScaler’: File exists\n",
            "mkdir: cannot create directory ‘test_data’: File exists\n",
            "mkdir: cannot create directory ‘specific’: File exists\n",
            "mkdir: cannot create directory ‘model’: File exists\n",
            "mkdir: cannot create directory ‘model/teams’: File exists\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "-GBW8rWknZnU"
      },
      "source": [
        "#Importing Libraries\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.externals import joblib\n",
        "from sklearn.preprocessing import OneHotEncoder"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "UX78lp13nZnV"
      },
      "source": [
        "def get_golden_feature_list(df,threshold=.5):\n",
        "    df_numeric = df.select_dtypes(include=[\"float64\",\"int64\"])\n",
        "    df_corr = df_numeric.corr()\n",
        "    df_corr = df_corr.iloc[2,:-1]\n",
        "    golden_features_list = df_corr[abs(df_corr) >= threshold].sort_values(ascending=False)\n",
        "    return golden_features_list\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NhvGRiPfnZnV"
      },
      "source": [
        "##### *get_golden_feature_list() : getting muchcorrelated features with total points which greater equal than threshold*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "cwTRp37XnZnW"
      },
      "source": [
        "def get_worst_feature_list(df,threshold=.5):\n",
        "    df_numeric = df.select_dtypes(include=[\"float64\",\"int64\"])\n",
        "    df_corr = df_numeric.corr()\n",
        "    df_corr = df_corr.iloc[2,:-1]\n",
        "    golden_features_list = df_corr[abs(df_corr) <= threshold].sort_values(ascending=False)\n",
        "    return golden_features_list\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RlLNgZDKnZnX"
      },
      "source": [
        "##### *get_worst_feature_list() : getting much uncorrelated features with total points which lower equal than threshold*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "gS6tPMdpnZnX"
      },
      "source": [
        "# Clean Up Columns Names\n",
        "def remove_whitespaces_in_df_columns(df):\n",
        "    df.columns = df.columns.str.replace(' ', '')\n",
        "    return df.columns"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "6JCLRorBnZnY"
      },
      "source": [
        "# Get Columns Names For All Data\n",
        "def get_column_names():\n",
        "    \"\"\"\n",
        "    :return: Column names of Data.csv\n",
        "    \"\"\"\n",
        "    feature_names_of_players = [\"PLAYER_NAME\", \"MIN\", \"FGM\", \"FGA\", \"FG_PCT\", \"FG3M\", \"FG3A\", \"FG3_PCT\", \"FTM\", \"FTA\",\n",
        "                                \"FT_PCT\", \"OREB\", \"DREB\", \"REB\", \"AST\", \"STL\", \"BLK\", \"TO\", \"PF\", \"PTS\", \"PLUS_MINUS\"]\n",
        "\n",
        "    feature_names_of_matchups = [\"gameId\", \"teamAbbr\", \"opptAbbr\", \"rslt\", \"teamMin\", \"teamPTS\", \"teamPTS1\", \"teamPTS2\",\n",
        "                                 \"teamPTS3\", \"teamPTS4\", \"opptPTS\",\n",
        "                                 \"opptPTS1\", \"opptPTS2\", \"opptPTS3\", \"opptPTS4\",\n",
        "                                 \"teamFGM\", \"teamFGA\", \"teamFG\", \"team3PM\", \"team3PA\", \"team3PCT\", \"teamFTM\", \"teamFTA\",\n",
        "                                 \"teamFTC\", \"teamORB\", \"teamDRB\", \"teamREB\", \"teamAST\", \"teamSTL\"\n",
        "        , \"teamBLK\", \"teamTO\", \"teamPF\", \"team2P\", \"teamTS\", \"teamEFG\", \"teamPPS\", \"teamFIC\", \"teamFIC40\", \"teamOrtg\",\n",
        "                                 \"teamDrtg\", \"teamPlay\",\n",
        "                                 \"opptMin\", \"opptFGM\", \"opptFGA\", \"opptFG\", \"oppt3PM\", \"oppt3PA\", \"oppt3PCT\", \"opptFTM\",\n",
        "                                 \"opptFTA\", \"opptFTC\", \"opptORB\", \"opptDRB\", \"opptREB\", \"opptAST\", \"opptSTL\"\n",
        "        , \"opptBLK\", \"opptTO\", \"opptPF\", \"oppt2P\", \"opptTS\", \"opptEFG\", \"opptPPS\", \"opptFIC\", \"opptFIC40\", \"opptOrtg\",\n",
        "                                 \"opptDrtg\", \"opptPlay\", \"poss\", \"pace\"]\n",
        "\n",
        "    team_features = [\"gameId\", \"teamAbbr\", \"opptAbbr\", \"rslt\", \"teamMin\", \"teamPTS\", \"teamPTS1\", \"teamPTS2\", \"teamPTS3\",\n",
        "                     \"teamPTS4\", \"opptPTS\",\n",
        "                     \"opptPTS1\", \"opptPTS2\", \"opptPTS3\", \"opptPTS4\",\n",
        "                     \"teamFGM\", \"teamFGA\", \"teamFG\", \"team3PM\", \"team3PA\", \"team3PCT\", \"teamFTM\", \"teamFTA\", \"teamFTC\",\n",
        "                     \"teamORB\", \"teamDRB\", \"teamREB\", \"teamAST\", \"teamSTL\"\n",
        "        , \"teamBLK\", \"teamTO\", \"teamPF\", \"team2P\", \"teamTS\", \"teamEFG\", \"teamPPS\", \"teamFIC\", \"teamFIC40\", \"teamOrtg\",\n",
        "                     \"teamDrtg\", \"teamPlay\"]\n",
        "    team_features = team_features + feature_names_of_players * 11\n",
        "\n",
        "    oppt_features = [\"opptMin\", \"opptFGM\", \"opptFGA\", \"opptFG\", \"oppt3PM\", \"oppt3PA\", \"oppt3PCT\", \"opptFTM\", \"opptFTA\",\n",
        "                     \"opptFTC\", \"opptORB\", \"opptDRB\", \"opptREB\", \"opptAST\", \"opptSTL\"\n",
        "        , \"opptBLK\", \"opptTO\", \"opptPF\", \"oppt2P\", \"opptTS\", \"opptEFG\", \"opptPPS\", \"opptFIC\", \"opptFIC40\", \"opptOrtg\",\n",
        "                     \"opptDrtg\", \"opptPlay\"]\n",
        "\n",
        "    oppt_features = oppt_features + feature_names_of_players * 11\n",
        "\n",
        "    last_features = [\"poss\", \"LM_totalPoint\",\"LM_dayOffset\",\"pace\"]\n",
        "\n",
        "    feature_names_of_matchups = team_features + oppt_features + last_features\n",
        "    return feature_names_of_matchups\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "hhDbTwn-nZnZ"
      },
      "source": [
        "# Getting unique team names \n",
        "def get_unique_team_abbr(df):\n",
        "    \"\"\"\n",
        "\n",
        "    :param df: Data.csv\n",
        "    :return: Team Abbrs\n",
        "    \"\"\"\n",
        "    return np.unique(df.teamAbbr.values)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "gm-gHDo6nZna"
      },
      "source": [
        "# Getting Unique Player List\n",
        "def get_unique_player_list(df):\n",
        "    \"\"\"\n",
        "    :param df: Data.csv\n",
        "    :return: Player List\n",
        "    \"\"\"\n",
        "    players = []\n",
        "    for i in range(df.PLAYER_NAME.shape[0]):\n",
        "        for j in range(df.PLAYER_NAME.shape[1]):\n",
        "            players.append(df.PLAYER_NAME.iloc[i, j])\n",
        "\n",
        "    unique_players = np.unique(players)\n",
        "    return unique_players\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "w3ghlThtnZnb"
      },
      "source": [
        "# Now,We write 3 methods for re-creating all player stats dataset.\n",
        "\n",
        "# First one is creating empty dataset to fill with real stats.\n",
        "\n",
        "def create_empty_player_stats(player_list):\n",
        "    \"\"\"\n",
        "\n",
        "    :param player_list: Oyuncu Listesi\n",
        "    :return: Oyuncu sayısı kadar row olan stats tablosu döndürür\n",
        "    \"\"\"\n",
        "    col_names_for_player_stats = [\"PLAYER_NAME\", \"MIN\", \"FGM\", \"FGA\", \"FG_PCT\", \"FG3M\", \"FG3A\", \"FG3_PCT\", \"FTM\", \"FTA\",\n",
        "                                  \"FT_PCT\", \"OREB\", \"DREB\", \"REB\", \"AST\", \"STL\", \"BLK\", \"TO\", \"PF\", \"PTS\", \"PLUS_MINUS\"]\n",
        "    index_for_player_stats = range(len(player_list))\n",
        "    player_stats_df = pd.DataFrame(columns=col_names_for_player_stats, index=index_for_player_stats)\n",
        "    return player_stats_df\n",
        "\n",
        "# Second one is splitting main data and getting player stats\n",
        "\n",
        "def get_all_players_stats_df(df):\n",
        "    \"\"\"\n",
        "\n",
        "    :param df: Data.csv --> Dataframe\n",
        "    :return:Bütün oyuncuların tüm maçlardaki verilerini döndürür\n",
        "    \"\"\"\n",
        "    team_player_first_column_index = df.columns.get_loc(\"teamPlay\") + 1\n",
        "    team_player_last_column_index = df.columns.get_loc(\"opptMin\")\n",
        "    oppt_player_first_column_index = df.columns.get_loc(\"opptPlay\") + 1\n",
        "    oppt_player_last_column_index = df.columns.get_loc(\"poss\")\n",
        "    team_player_stats = df.iloc[:, team_player_first_column_index:team_player_last_column_index]\n",
        "    oppt_player_stats = df.iloc[:, oppt_player_first_column_index:oppt_player_last_column_index]\n",
        "    player_stats_conc = pd.concat([team_player_stats, oppt_player_stats], axis=0)\n",
        "    player_stats_conc_splited = np.array_split(player_stats_conc, 11, axis=1)\n",
        "    full_df = pd.concat([player_stats_conc_splited[0],\n",
        "                         player_stats_conc_splited[1],\n",
        "                         player_stats_conc_splited[2],\n",
        "                         player_stats_conc_splited[3],\n",
        "                         player_stats_conc_splited[4],\n",
        "                         player_stats_conc_splited[5],\n",
        "                         player_stats_conc_splited[6],\n",
        "                         player_stats_conc_splited[7],\n",
        "                         player_stats_conc_splited[8],\n",
        "                         player_stats_conc_splited[9],\n",
        "                         player_stats_conc_splited[10]], axis=0)\n",
        "    return full_df\n",
        "\n",
        "# Last one is filling empty dataframe with these datas\n",
        "# Last x Match variable is how many match ago\n",
        "\n",
        "def get_all_player_stats_last_x_match(all_player_stats_df,all_player_stats_empty_df,player_list,last_x_match=10):\n",
        "    \"\"\"\n",
        "\n",
        "    :param all_player_stats_df: Maçlara göre olan bütün oyuncuların verileri\n",
        "    :param all_player_stats_empty_df: Player sayısı kadar row olan boş veri tablosu\n",
        "    :param player_list: Playerların listesi\n",
        "    :param last_x_match: Son kaç maç olduğu\n",
        "    :return: Bütün oyuncuların ortalama verileri\n",
        "    \"\"\"\n",
        "    for i in range(len(player_list)):\n",
        "        each = player_list[i]\n",
        "        player_stats_each = all_player_stats_df[all_player_stats_df.PLAYER_NAME == each][-1:-last_x_match:-1]\n",
        "        player_name = each\n",
        "        mean_of_stats_this_player = player_stats_each.drop(columns=[\"PLAYER_NAME\"], axis=1).mean()\n",
        "        all_player_stats_empty_df.at[i, \"PLAYER_NAME\"] = player_name\n",
        "        all_player_stats_empty_df.iloc[i, 1:] = mean_of_stats_this_player\n",
        "    try :\n",
        "        all_player_stats_empty_df.to_csv(\"inputs/all_player_stats.csv\")\n",
        "        print(\"CSV Saved To Input Folder name all_player_stats.csv\")\n",
        "    except Exception as e:\n",
        "        print(e)\n",
        "    return all_player_stats_empty_df\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "cpHeoUyynZnc"
      },
      "source": [
        "# If you want to get just one player by name you can use this method\n",
        "# All player stats df is dataframe which get_all_player_stats_last_x_match return\n",
        "def get_player_stats_by_name(all_player_stats_df,player_name,last_x_match=10):\n",
        "    \"\"\"\n",
        "\n",
        "    :param all_player_stats_df: Bütün playerların maçlara göre verileri\n",
        "    :param player_name: Player ismi\n",
        "    :param last_x_match: Son kaç maç olduğu\n",
        "    :return: Playerın ortalama değerleri\n",
        "    \"\"\"\n",
        "\n",
        "    player_stats_each = all_player_stats_df[all_player_stats_df.PLAYER_NAME == player_name][-1:-last_x_match:-1]\n",
        "    mean_of_stats_this_player = player_stats_each.drop(columns=[\"PLAYER_NAME\"], axis=1).mean()\n",
        "    # all_player_stats_empty_df.at[i, \"PLAYER_NAME\"] = player_name\n",
        "    # all_player_stats_empty_df.iloc[i, 1:] = mean_of_stats_this_player\n",
        "\n",
        "    return player_name,mean_of_stats_this_player\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "-hEMfiZRnZnc"
      },
      "source": [
        "# Now we create dataframe which includes every teams matches with each others.\n",
        "# df is main dataframe\n",
        "def get_play_by_play_stats(df,last_x_match=10):\n",
        "    \"\"\"\n",
        "\n",
        "    :param df: Data.csv\n",
        "    :param last_x_match: Son kaç maç\n",
        "    :return: Takımların diğer takımlarla olan maç istatistikleri\n",
        "    \"\"\"\n",
        "\n",
        "    \"\"\"preparing dropped columns\"\"\"\n",
        "\n",
        "    teamAbbr_unique = get_unique_team_abbr(df)\n",
        "    opptAbbr_unique = get_unique_team_abbr(df)\n",
        "    coll = df.drop(df.columns[42:272],axis=1)\n",
        "    coll = df.drop(df.columns[300:530],axis=1)\n",
        "    coll = coll.drop(\"gameId\",axis=1)\n",
        "    df = coll\n",
        "\n",
        "    play_by_play = pd.DataFrame(columns=df.columns, index=range(len(teamAbbr_unique) * len(teamAbbr_unique)))\n",
        "    i = 0\n",
        "    for each in teamAbbr_unique:\n",
        "        for k in opptAbbr_unique:\n",
        "            if each != k:\n",
        "                if df[(df.teamAbbr == each) & (df.opptAbbr == k)].empty == False:\n",
        "                    df_for_each = df[(df.teamAbbr == each) & (df.opptAbbr == k)][-1:-last_x_match:-1]\n",
        "\n",
        "                    play_by_play_count = df_for_each[\"rslt\"].value_counts()\n",
        "                    # print(each,k)\n",
        "                    # print(df[(df.teamAbbr == each) & (df.opptAbbr == k)])\n",
        "                    if play_by_play_count.shape[0] == 1:\n",
        "                        play_by_play_count = play_by_play_count[0] / (play_by_play_count[0]) * 100\n",
        "                    else:\n",
        "                        play_by_play_count = play_by_play_count[0] / (play_by_play_count[0] + play_by_play_count[1]) * 100\n",
        "                    play_by_play.iloc[i, 3:] = df[(df.teamAbbr == each) & (df.opptAbbr == k)].iloc[:, 3:].mean()\n",
        "                    play_by_play.loc[i, \"teamAbbr\"] = each\n",
        "                    play_by_play.loc[i, \"opptAbbr\"] = k\n",
        "                    play_by_play.at[i, \"rslt\"] = play_by_play_count\n",
        "\n",
        "                    i += 1\n",
        "                else:\n",
        "                    pass\n",
        "    try :\n",
        "        play_by_play.to_csv(\"inputs/play_by_play.csv\")\n",
        "        print(\"CSV Saved To Input Folder name play_by_play.csv\")\n",
        "    except Exception as e:\n",
        "        print(e)\n",
        "    return play_by_play\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "6CC_mo08nZnd"
      },
      "source": [
        "# If you want to use any match in your model you can create one row data with this method.\n",
        "\n",
        "# teamAbbr = home Team\n",
        "# opptAbbr = away Team\n",
        "# data_csv_name = Main data path\n",
        "# Player stats csv name = all_player_stats path\n",
        "def get_test_data(teamAbbr,opptAbbr,homeplayers,awayplayers,data_csv_name = \"/content/inputs/matches_w_player_stats.csv\",player_stats_csv_name = \"../input/all_player_stats.csv\",last_x_match=10):\n",
        "    df = pd.read_csv(data_csv_name)\n",
        "    df.columns = remove_whitespaces_in_df_columns(df)  # clean column names\n",
        "    df.columns = get_column_names()  # get column names and assign\n",
        "    all_players_stats_df = pd.read_csv(player_stats_csv_name,index_col=0)\n",
        "    play_by_play = get_play_by_play_stats(df,last_x_match)\n",
        "    test_data = play_by_play[(play_by_play[\"teamAbbr\"] == teamAbbr) & (play_by_play[\"opptAbbr\"] == opptAbbr)]\n",
        "    home = []\n",
        "    away = []\n",
        "    feature_names_of_players = [\"MIN\", \"FGM\", \"FGA\", \"FG_PCT\", \"FG3M\", \"FG3A\", \"FG3_PCT\", \"FTM\", \"FTA\",\n",
        "                                \"FT_PCT\", \"OREB\", \"DREB\", \"REB\", \"AST\", \"STL\", \"BLK\", \"TO\", \"PF\", \"PTS\", \"PLUS_MINUS\"]\n",
        "    for each in homeplayers:\n",
        "        player_name , mean_of_player = get_player_stats_by_name(all_player_stats_df=all_players_stats_df,player_name=each,last_x_match=last_x_match)\n",
        "        mean_of_player = mean_of_player.tolist()\n",
        "        home.append(mean_of_player)\n",
        "    for each in awayplayers:\n",
        "        player_name , mean_of_player = get_player_stats_by_name(all_player_stats_df=all_players_stats_df,player_name=each,last_x_match=last_x_match)\n",
        "        mean_of_player = mean_of_player.tolist()\n",
        "        away.append(mean_of_player)\n",
        "    all_df = pd.DataFrame(columns=range(len(feature_names_of_players)*(len(homeplayers)+len(awayplayers))),index=range(1))\n",
        "    start_offset = 0\n",
        "    end_offset = len(feature_names_of_players)\n",
        "    for i in range(len(homeplayers)):\n",
        "        all_df.iloc[0,start_offset:end_offset] = home[i]\n",
        "        start_offset = end_offset\n",
        "        end_offset += 20\n",
        "    start_offset = len(homeplayers) * len(feature_names_of_players)\n",
        "    end_offset = start_offset + len(feature_names_of_players)\n",
        "    for i in range(len(awayplayers)):\n",
        "        all_df.iloc[0,start_offset:end_offset] = away[i]\n",
        "        start_offset = end_offset\n",
        "        end_offset += len(feature_names_of_players)\n",
        "    all_df.columns = feature_names_of_players * (len(homeplayers) + len(awayplayers))\n",
        "    test_data.reset_index(inplace=True)\n",
        "\n",
        "    conc =  pd.concat([test_data,all_df],axis=1)\n",
        "    conc.to_csv(\"test_data/\"+teamAbbr+\"vs\"+opptAbbr+\"_test_data.csv\")\n",
        "    conc.fillna(value=0,inplace=True)\n",
        "    conc.drop(\"index\",axis=1,inplace=True)\n",
        "    return conc\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4hRLhtTHnZnf"
      },
      "source": [
        "### Now we preprocessed our data.Time to label encoding , one hot encoding , scaling."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "4svmsTTenZng"
      },
      "source": [
        "# Dropping un neccesary columns for training.\n",
        "def drop_some_columns(df):\n",
        "    df = df.drop(columns=[\"PLAYER_NAME\"],axis=1)\n",
        "    df = df.drop(columns=[\"gameId\"],axis=1)\n",
        "    return df\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "kAuKmDzOnZng"
      },
      "source": [
        "# Label Encoding all data and saving label encoder.\n",
        "def label_train_data(df):\n",
        "    le = LabelEncoder()\n",
        "    df[\"teamAbbr\"] = le.fit_transform(df[\"teamAbbr\"])\n",
        "    keys = le.classes_\n",
        "    values = le.transform(le.classes_)\n",
        "    dictionary1 = dict(zip(keys, values))\n",
        "    np.save('labelencoder/team_abbr.npy', le.classes_)\n",
        "    df[\"opptAbbr\"] = le.fit_transform(df[\"opptAbbr\"])\n",
        "    keys = le.classes_\n",
        "    values = le.transform(le.classes_)\n",
        "    np.save('labelencoder/oppt_abbr.npy', le.classes_)\n",
        "    dictionary2 = dict(zip(keys, values))\n",
        "    df[\"rslt\"] = le.fit_transform(df[\"rslt\"])\n",
        "    keys = le.classes_\n",
        "    values = le.transform(le.classes_)\n",
        "    np.save('labelencoder/rslt.npy', le.classes_)\n",
        "    dictionary3 = dict(zip(keys, values))\n",
        "    return df,dictionary1,dictionary2,dictionary3\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "yOxujyYGnZnh"
      },
      "source": [
        "# Same for test data.Loading and Encoding.\n",
        "def label_test_data(df):\n",
        "    encoder = LabelEncoder()\n",
        "    encoder.classes_ = np.load('labelencoder/team_abbr.npy',allow_pickle=True)\n",
        "    df.teamAbbr = encoder.transform(df.teamAbbr)\n",
        "    encoder = LabelEncoder()\n",
        "    encoder.classes_ = np.load('labelencoder/oppt_abbr.npy',allow_pickle=True)\n",
        "    df.opptAbbr = encoder.transform(df.opptAbbr)\n",
        "\n",
        "    return df\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "tOJPuGENnZni"
      },
      "source": [
        "# For cleaning Nan Values use this function.\n",
        "def clean_nan_values(df):\n",
        "    df.dropna(inplace=True)\n",
        "    return df\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "bB6T9vT2nZni"
      },
      "source": [
        "# Scaling All Data and saving standart scaler.\n",
        "\n",
        "def standart_scaler_all_data(df):\n",
        "    scaler_filename = \"standartScaler/scaler.bin\"\n",
        "    ss = StandardScaler()\n",
        "    df.iloc[:, 3:] = ss.fit_transform(df.iloc[:, 3:])\n",
        "    joblib.dump(ss, scaler_filename,compress=True)\n",
        "    print(\"Saved Standart Scaler File to \",scaler_filename)\n",
        "    return df,ss\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "2RcNGOoFnZni"
      },
      "source": [
        "# Same for test data load and scale.\n",
        "def standart_scaler_test_data(df):\n",
        "    scaler_filename = \"standartScaler/scaler.bin\"\n",
        "    scaler = joblib.load(scaler_filename)\n",
        "    df.iloc[:, 3:] = scaler.transform(df.iloc[:, 3:])\n",
        "    return df\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "X4_pR-IAnZni"
      },
      "source": [
        "# One hot encoding required columns and save it.\n",
        "def onehotencoder_all_data(df):\n",
        "    ohe = OneHotEncoder(categorical_features=[0, 1], n_values='auto',\n",
        "                        handle_unknown='ignore')\n",
        "\n",
        "    df = ohe.fit_transform(df).toarray()\n",
        "    df = pd.DataFrame(df)\n",
        "    onehotencoder_name = \"onehotencoder/onehotencoder.bin\"\n",
        "    joblib.dump(ohe, onehotencoder_name, compress=True)\n",
        "    print(\"Saved ohe to \",onehotencoder_name)\n",
        "    return df\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "4rCd3TYnnZnj"
      },
      "source": [
        "#Same for test data load and encode.\n",
        "def onehotencoder_test_data(df):\n",
        "    onehotencoder_name = \"onehotencoder/onehotencoder.bin\"\n",
        "    ohe = joblib.load(onehotencoder_name)\n",
        "    df = ohe.transform(df).toarray()\n",
        "    return pd.DataFrame(df)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "VviTX9PSnZnj"
      },
      "source": [
        "#You must use this method because some teams is closed in NBA therefore we don't have to use them for training.\n",
        "def old_to_new_team_abbrs(df):\n",
        "    currently_available_teams = {}\n",
        "    currently_available_teams[\"NJN\"] = \"BKN\" # old to new\n",
        "    currently_available_teams[\"NOH\"] = \"NOP\" # old to new\n",
        "\n",
        "    non_teams = ['EST', 'FLA', 'GNS', 'GUA', 'MAC']\n",
        "    for key, value in currently_available_teams.items():\n",
        "        df['teamAbbr'] = df['teamAbbr'].str.replace(key, value)\n",
        "        df[\"opptAbbr\"] = df[\"opptAbbr\"].str.replace(key, value)\n",
        "    for each in non_teams:\n",
        "        drop_index = df[(df.teamAbbr == each) | (df.opptAbbr == each)].index\n",
        "        df.drop(drop_index, inplace=True)\n",
        "    print(get_unique_team_abbr(df))\n",
        "    return df\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "4U-2rS_VnZnj"
      },
      "source": [
        "#This method is doing same job with re-creating all player stats\n",
        "#df is main dataframe\n",
        "#players is player list \n",
        "def get_one_shot_player_stats(players,df,last_x_match):\n",
        "    empty_stats_df = create_empty_player_stats(players)  # create empty stats of teams table\n",
        "    all_players_stats_df = get_all_players_stats_df(df)  # assign all team stats to empty table\n",
        "    all_players_stats_last_x_match = get_all_player_stats_last_x_match(all_player_stats_df=all_players_stats_df, all_player_stats_empty_df=empty_stats_df,player_list= players,last_x_match=last_x_match)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "uAqqfmtNnZnj"
      },
      "source": [
        "#This one is not include parameter.\n",
        "#You can use by giving csv urls.\n",
        "def get_available_player_stats():\n",
        "    csv_url=\"../input/matches_w_player_stats.csv\"\n",
        "    df = pd.read_csv(csv_url,header=None)\n",
        "    df.columns = get_column_names()\n",
        "    play_by_play = get_play_by_play_stats(df,last_x_match=1000)\n",
        "    players = pd.read_csv(\"../input/player_list.csv\",header=None)\n",
        "    players = players.values\n",
        "    players = players[:,0]\n",
        "    get_one_shot_player_stats(players,df)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vM-tMrQHnZnj"
      },
      "source": [
        "# 2-Building Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pQUqSGT5nuoF",
        "outputId": "e74d5fa6-5bb9-4df3-8d6a-e0ce3b34f74d"
      },
      "source": [
        "pip install preprocessing"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: preprocessing in /usr/local/lib/python3.6/dist-packages (0.1.13)\n",
            "Requirement already satisfied: nltk==3.2.4 in /usr/local/lib/python3.6/dist-packages (from preprocessing) (3.2.4)\n",
            "Requirement already satisfied: sphinx-rtd-theme==0.2.4 in /usr/local/lib/python3.6/dist-packages (from preprocessing) (0.2.4)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from nltk==3.2.4->preprocessing) (1.15.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "fMq6Kr-vnZnk"
      },
      "source": [
        "#Importing LIBS\n",
        "\n",
        "from preprocessing import *\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Model\n",
        "from xgboost import XGBClassifier,XGBRegressor,Booster\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.externals import joblib\n",
        "import pickle\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.model_selection import RandomizedSearchCV, GridSearchCV\n",
        "from sklearn.metrics import roc_auc_score,regression\n",
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "from sklearn.ensemble import AdaBoostClassifier,RandomForestClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.naive_bayes import  GaussianNB\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "from sklearn.ensemble import GradientBoostingRegressor,RandomForestRegressor,BaggingRegressor\n",
        "from sklearn.neighbors import KNeighborsRegressor,RadiusNeighborsRegressor\n",
        "from sklearn.ensemble import AdaBoostRegressor,ExtraTreesRegressor\n",
        "from sklearn.gaussian_process import GaussianProcessRegressor\n",
        "from sklearn.linear_model import SGDRegressor\n",
        "from sklearn.svm import SVR,LinearSVR,NuSVR"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KP_4toDJnZnk"
      },
      "source": [
        "# 3-Calling Preprocess Methods for our data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "qdk4L5QDnZnk"
      },
      "source": [
        "#Preprocessing for predicting match result\n",
        "\n",
        "def preprocess(csv_url = \"/content/inputs/matches_w_player_stats.csv\"):\n",
        "    df = pd.read_csv(csv_url)  # read data\n",
        "    df.columns = remove_whitespaces_in_df_columns(df)  # clean column names\n",
        "    df.columns = get_column_names()  # get column names and assign\n",
        "    df = old_to_new_team_abbrs(df)\n",
        "    # players = get_unique_player_list(df)  # get all player list\n",
        "    # team_abbrs = get_unique_team_abbr(df)  # get all team names\n",
        "    df = drop_some_columns(df)\n",
        "    df = clean_nan_values(df)\n",
        "    Y = np.array(df.rslt)\n",
        "    encoded,dict1,dict2,dict3 = label_train_data(df)\n",
        "    scalered ,ss= standart_scaler_all_data(encoded)\n",
        "    df = onehotencoder_all_data(scalered)\n",
        "    return df,Y\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BvtBI2pxnZnk"
      },
      "source": [
        "# 4-Preprocessing for predicting home and away point\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "vLY_xkAunZnl"
      },
      "source": [
        "\n",
        "def preprocess_home_team_point(csv_url =\"/content/inputs/matches_w_player_stats.csv\"):\n",
        "    df = pd.read_csv(csv_url)  # read data\n",
        "    df.columns = remove_whitespaces_in_df_columns(df)  # clean column names\n",
        "    df.columns = get_column_names()  # get column names and assign\n",
        "    df = old_to_new_team_abbrs(df)\n",
        "    # players = get_unique_player_list(df)  # get all player list\n",
        "    # team_abbrs = get_unique_team_abbr(df)  # get all team names\n",
        "    df = drop_some_columns(df)\n",
        "    df = clean_nan_values(df)\n",
        "    Y = np.array(df.teamPTS)\n",
        "    encoded,dict1,dict2,dict3 = label_train_data(df)\n",
        "    scalered ,ss= standart_scaler_all_data(encoded)\n",
        "    df = onehotencoder_all_data(scalered)\n",
        "    return df,Y\n",
        "def preprocess_away_team_point(csv_url =\"/content/inputs/matches_w_player_stats.csv\"):\n",
        "    df = pd.read_csv(csv_url)  # read data\n",
        "    df.columns = remove_whitespaces_in_df_columns(df)  # clean column names\n",
        "    df.columns = get_column_names()  # get column names and assign\n",
        "    df = old_to_new_team_abbrs(df)\n",
        "    # players = get_unique_player_list(df)  # get all player list\n",
        "    # team_abbrs = get_unique_team_abbr(df)  # get all team names\n",
        "    df = drop_some_columns(df)\n",
        "    df = clean_nan_values(df)\n",
        "    Y = np.array(df.opptPTS)\n",
        "    encoded,dict1,dict2,dict3 = label_train_data(df)\n",
        "    scalered ,ss= standart_scaler_all_data(encoded)\n",
        "    df = onehotencoder_all_data(scalered)\n",
        "    return df,Y"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oK_tJSvjnZnl"
      },
      "source": [
        "# 5-Preprocessing for predicting home quarter's point\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "FhVO_ASCnZnl"
      },
      "source": [
        "\n",
        "def preprocess_home_q1_point(csv_url):\n",
        "    df = pd.read_csv(csv_url)  # read data\n",
        "    df.columns = remove_whitespaces_in_df_columns(df)  # clean column names\n",
        "    df.columns = get_column_names()  # get column names and assign\n",
        "    df = old_to_new_team_abbrs(df)\n",
        "    # players = get_unique_player_list(df)  # get all player list\n",
        "    # team_abbrs = get_unique_team_abbr(df)  # get all team names\n",
        "    df = drop_some_columns(df)\n",
        "    df = clean_nan_values(df)\n",
        "    Y = np.array(df.teamPTS1)\n",
        "    encoded,dict1,dict2,dict3 = label_train_data(df)\n",
        "    scalered ,ss= standart_scaler_all_data(encoded)\n",
        "    df = onehotencoder_all_data(scalered)\n",
        "    return df,Y\n",
        "def preprocess_home_q2_point(csv_url):\n",
        "    df = pd.read_csv(csv_url)  # read data\n",
        "    df.columns = remove_whitespaces_in_df_columns(df)  # clean column names\n",
        "    df.columns = get_column_names()  # get column names and assign\n",
        "    df = old_to_new_team_abbrs(df)\n",
        "    # players = get_unique_player_list(df)  # get all player list\n",
        "    # team_abbrs = get_unique_team_abbr(df)  # get all team names\n",
        "    df = drop_some_columns(df)\n",
        "    df = clean_nan_values(df)\n",
        "    Y = np.array(df.teamPTS2)\n",
        "    encoded,dict1,dict2,dict3 = label_train_data(df)\n",
        "    scalered ,ss= standart_scaler_all_data(encoded)\n",
        "    df = onehotencoder_all_data(scalered)\n",
        "    return df,Y\n",
        "def preprocess_home_q3_point(csv_url):\n",
        "    df = pd.read_csv(csv_url)  # read data\n",
        "    df.columns = remove_whitespaces_in_df_columns(df)  # clean column names\n",
        "    df.columns = get_column_names()  # get column names and assign\n",
        "    df = old_to_new_team_abbrs(df)\n",
        "    # players = get_unique_player_list(df)  # get all player list\n",
        "    # team_abbrs = get_unique_team_abbr(df)  # get all team names\n",
        "    df = drop_some_columns(df)\n",
        "    df = clean_nan_values(df)\n",
        "    Y = np.array(df.teamPTS3)\n",
        "    encoded,dict1,dict2,dict3 = label_train_data(df)\n",
        "    scalered ,ss= standart_scaler_all_data(encoded)\n",
        "    df = onehotencoder_all_data(scalered)\n",
        "    return df,Y\n",
        "def preprocess_home_q4_point(csv_url):\n",
        "    df = pd.read_csv(csv_url)  # read data\n",
        "    df.columns = remove_whitespaces_in_df_columns(df)  # clean column names\n",
        "    df.columns = get_column_names()  # get column names and assign\n",
        "    df = old_to_new_team_abbrs(df)\n",
        "    # players = get_unique_player_list(df)  # get all player list\n",
        "    # team_abbrs = get_unique_team_abbr(df)  # get all team names\n",
        "    df = drop_some_columns(df)\n",
        "    df = clean_nan_values(df)\n",
        "    Y = np.array(df.teamPTS4)\n",
        "    encoded,dict1,dict2,dict3 = label_train_data(df)\n",
        "    scalered ,ss= standart_scaler_all_data(encoded)\n",
        "    df = onehotencoder_all_data(scalered)\n",
        "    return df,Y"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZqUczk2BnZnl"
      },
      "source": [
        "# 6-Preprocessing for predicting away quarter's point\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "vTOYCNcKnZnl"
      },
      "source": [
        "\n",
        "def preprocess_away_q1_point(csv_url):\n",
        "    df = pd.read_csv(csv_url)  # read data\n",
        "    df.columns = remove_whitespaces_in_df_columns(df)  # clean column names\n",
        "    df.columns = get_column_names()  # get column names and assign\n",
        "    df = old_to_new_team_abbrs(df)\n",
        "    # players = get_unique_player_list(df)  # get all player list\n",
        "    # team_abbrs = get_unique_team_abbr(df)  # get all team names\n",
        "    df = drop_some_columns(df)\n",
        "    df = clean_nan_values(df)\n",
        "    Y = np.array(df.opptPTS1)\n",
        "    encoded,dict1,dict2,dict3 = label_train_data(df)\n",
        "    scalered ,ss= standart_scaler_all_data(encoded)\n",
        "    df = onehotencoder_all_data(scalered)\n",
        "    return df,Y\n",
        "def preprocess_away_q2_point(csv_url):\n",
        "    df = pd.read_csv(csv_url)  # read data\n",
        "    df.columns = remove_whitespaces_in_df_columns(df)  # clean column names\n",
        "    df.columns = get_column_names()  # get column names and assign\n",
        "    df = old_to_new_team_abbrs(df)\n",
        "    # players = get_unique_player_list(df)  # get all player list\n",
        "    # team_abbrs = get_unique_team_abbr(df)  # get all team names\n",
        "    df = drop_some_columns(df)\n",
        "    df = clean_nan_values(df)\n",
        "    Y = np.array(df.opptPTS2)\n",
        "    encoded,dict1,dict2,dict3 = label_train_data(df)\n",
        "    scalered ,ss= standart_scaler_all_data(encoded)\n",
        "    df = onehotencoder_all_data(scalered)\n",
        "    return df,Y\n",
        "def preprocess_away_q3_point(csv_url):\n",
        "    df = pd.read_csv(csv_url)  # read data\n",
        "    df.columns = remove_whitespaces_in_df_columns(df)  # clean column names\n",
        "    df.columns = get_column_names()  # get column names and assign\n",
        "    df = old_to_new_team_abbrs(df)\n",
        "    # players = get_unique_player_list(df)  # get all player list\n",
        "    # team_abbrs = get_unique_team_abbr(df)  # get all team names\n",
        "    df = drop_some_columns(df)\n",
        "    df = clean_nan_values(df)\n",
        "    Y = np.array(df.opptPTS3)\n",
        "    encoded,dict1,dict2,dict3 = label_train_data(df)\n",
        "    scalered ,ss= standart_scaler_all_data(encoded)\n",
        "    df = onehotencoder_all_data(scalered)\n",
        "    return df,Y\n",
        "def preprocess_away_q4_point(csv_url):\n",
        "    df = pd.read_csv(csv_url)  # read data\n",
        "    df.columns = remove_whitespaces_in_df_columns(df)  # clean column names\n",
        "    df.columns = get_column_names()  # get column names and assign\n",
        "    df = old_to_new_team_abbrs(df)\n",
        "    # players = get_unique_player_list(df)  # get all player list\n",
        "    # team_abbrs = get_unique_team_abbr(df)  # get all team names\n",
        "    df = drop_some_columns(df)\n",
        "    df = clean_nan_values(df)\n",
        "    Y = np.array(df.opptPTS4)\n",
        "    encoded,dict1,dict2,dict3 = label_train_data(df)\n",
        "    scalered ,ss= standart_scaler_all_data(encoded)\n",
        "    df = onehotencoder_all_data(scalered)\n",
        "    return df,Y"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q_1vqMj-nZnm"
      },
      "source": [
        "# 7-Splitting data to Data and Label"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "zL2P7KrfnZnm"
      },
      "source": [
        "# For this,We use seperate methods to split data because we have 11 model.\n",
        "def split_data_by_result(df,result_index=60):\n",
        "    X = df.drop(columns=result_index)\n",
        "    Y = df[result_index]\n",
        "    print(Y)\n",
        "    return X,Y\n",
        "def split_data_home_point(df, result_index=62):\n",
        "    X = df.drop(columns=result_index)\n",
        "    Y = df[result_index]\n",
        "    return X, Y\n",
        "def split_data_away_point(df, result_index=67):\n",
        "    X = df.drop(columns=result_index)\n",
        "    Y = df[result_index]\n",
        "    return X, Y\n",
        "def split_data_home_q1_point(df, result_index=63):\n",
        "    X = df.drop(columns=result_index)\n",
        "    Y = df[result_index]\n",
        "    return X, Y\n",
        "def split_data_home_q2_point(df, result_index=64):\n",
        "    X = df.drop(columns=result_index)\n",
        "    Y = df[result_index]\n",
        "    return X, Y\n",
        "def split_data_home_q3_point(df, result_index=65):\n",
        "    X = df.drop(columns=result_index)\n",
        "    Y = df[result_index]\n",
        "    return X, Y\n",
        "def split_data_home_q4_point(df, result_index=66):\n",
        "    X = df.drop(columns=result_index)\n",
        "    Y = df[result_index]\n",
        "    return X, Y\n",
        "def split_data_away_q1_point(df, result_index=68):\n",
        "    X = df.drop(columns=result_index)\n",
        "    Y = df[result_index]\n",
        "    return X, Y\n",
        "def split_data_away_q2_point(df, result_index=69):\n",
        "    X = df.drop(columns=result_index)\n",
        "    Y = df[result_index]\n",
        "    return X, Y\n",
        "def split_data_away_q3_point(df, result_index=70):\n",
        "    X = df.drop(columns=result_index)\n",
        "    Y = df[result_index]\n",
        "    return X, Y\n",
        "def split_data_away_q4_point(df, result_index=71):\n",
        "    X = df.drop(columns=result_index)\n",
        "    Y = df[result_index]\n",
        "    return X, Y\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YsQUXZtqnZnm"
      },
      "source": [
        "## 8-Building Models based XGBOOST"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "_tMNGLQUnZnm"
      },
      "source": [
        "#Build classification model and save\n",
        "def build_classifier_model(X_train,Y_train,X_test,Y_test):\n",
        "    # model = XGBClassifier(n_estimators=5000,nthread=4,seed=42,reg_lambda=0.95,reg_alpha=0.45,tree_method=\"gpu_hist\",max_depth=3,objective=\"binary:logistic\")\n",
        "    model = XGBClassifier(tree_method=\"gpu_hist\",nthread=4,n_estimators=1000)\n",
        "\n",
        "    model.fit(X_train, Y_train)\n",
        "    pickle.dump(model,open(\"model/xgb_classifier.pkl\",\"wb\"))\n",
        "    print(model.score(X_test,Y_test))\n",
        "    print(model.classes_)\n",
        "\n",
        "    return model , model.score(X_test,Y_test)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "ZvbqunNInZnn"
      },
      "source": [
        "def build_class_model(csv_url=\"../input/matches_w_player_stats.csv\"):\n",
        "    df,Y=preprocess(csv_url)\n",
        "\n",
        "    X,_ = split_data_by_result(df=df)\n",
        "    X_train,X_test,Y_train,Y_test = data_split(X,Y)\n",
        "    model_ , score__ = build_classifier_model(X_train,Y_train,X_test,Y_test)\n",
        "    return model_,X_train,X_test,Y_train,Y_test\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "l_7a_lVynZnn"
      },
      "source": [
        "def build_regressor_model(X_train, Y_train, X_test, Y_test, name):\n",
        "    # model = XGBRegressor(learning_rate=0.15,gamma=0,reg_lambda=0.01,max_delta_step=0, max_depth=3,n_estimators=10000,\n",
        "    #                      min_child_weight=1,nthread=4,tree_method=\"gpu_hist\")\n",
        "    model = XGBRegressor(tree_method=\"gpu_hist\",n_estimators=1000,nthread=4)\n",
        "    model.fit(X_train, Y_train)\n",
        "    pickle.dump(model, open(name, \"wb\"))\n",
        "    print(model.score(X_test,Y_test))\n",
        "    return model, model.score(X_test, Y_test)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "fOk4_9WInZnn"
      },
      "source": [
        "def build_home_point_predict_model(csv_url=\"../input/matches_w_player_stats.csv\"):\n",
        "    df,Y=preprocess_home_team_point(csv_url)\n",
        "    X,_ = split_data_home_point(df=df)\n",
        "    X_train,X_test,Y_train,Y_test = data_split_regresyon(X,Y)\n",
        "    model_ , score__ = build_regressor_model(X_train=X_train,Y_train=Y_train,X_test=X_test,Y_test=Y_test,\n",
        "                                             name=\"model/teams/home_point_model.pkl\")\n",
        "    return model_,X_train,X_test,Y_train,Y_test\n",
        "def build_away_point_predict_model(csv_url=\"../input/matches_w_player_stats.csv\"):\n",
        "    df,Y=preprocess_away_team_point(csv_url)\n",
        "    X,_ = split_data_away_point(df=df)\n",
        "    X_train,X_test,Y_train,Y_test = data_split_regresyon(X,Y)\n",
        "    model_ , score__ = build_regressor_model(X_train=X_train,Y_train=Y_train,X_test=X_test,Y_test=Y_test,name=\"model/teams/away_point_model.pkl\")\n",
        "    return model_,X_train,X_test,Y_train,Y_test\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "ekZwhtf2nZnn"
      },
      "source": [
        "def build_home_q1_point_predict_model(csv_url=\"../input/matches_w_player_stats.csv\"):\n",
        "    df,Y=preprocess_home_q1_point(csv_url)\n",
        "    X,_ = split_data_home_q1_point(df=df)\n",
        "    X_train,X_test,Y_train,Y_test = data_split_regresyon(X,Y)\n",
        "    model_ , score__ = build_regressor_model(X_train,Y_train,X_test,Y_test,name=\"model/teams/home_q1_point_model.pkl\")\n",
        "    return model_,X_train,X_test,Y_train,Y_test\n",
        "def build_home_q2_point_predict_model(csv_url=\"../input/fulldata/full_data.csv\"):\n",
        "    df,Y=preprocess_home_q2_point(csv_url)\n",
        "    X,_ = split_data_home_q2_point(df=df)\n",
        "    X_train,X_test,Y_train,Y_test = data_split_regresyon(X,Y)\n",
        "    model_ , score__ = build_regressor_model(X_train,Y_train,X_test,Y_test,name=\"model/teams/home_q2_point_model.pkl\")\n",
        "    return model_,X_train,X_test,Y_train,Y_test\n",
        "def build_home_q3_point_predict_model(csv_url=\"../input/matches_w_player_stats.csv\"):\n",
        "    df,Y=preprocess_home_q3_point(csv_url)\n",
        "    X,_ = split_data_home_q3_point(df=df)\n",
        "    X_train,X_test,Y_train,Y_test = data_split_regresyon(X,Y)\n",
        "    model_ , score__ = build_regressor_model(X_train,Y_train,X_test,Y_test,name=\"model/teams/home_q3_point_model.pkl\")\n",
        "    return model_,X_train,X_test,Y_train,Y_test\n",
        "def build_home_q4_point_predict_model(csv_url=\"../input/matches_w_player_stats.csv\"):\n",
        "    df,Y=preprocess_home_q4_point(csv_url)\n",
        "    X,_ = split_data_home_q4_point(df=df)\n",
        "    X_train,X_test,Y_train,Y_test = data_split_regresyon(X,Y)\n",
        "    model_ , score__ = build_regressor_model(X_train,Y_train,X_test,Y_test,name=\"model/teams/home_q4_point_model.pkl\")\n",
        "    return model_,X_train,X_test,Y_train,Y_test"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "0Q4lV4VjnZnn"
      },
      "source": [
        "def build_away_q1_point_predict_model(csv_url=\"../input/matches_w_player_stats.csv\"):\n",
        "    df,Y=preprocess_away_q1_point(csv_url)\n",
        "    X,_ = split_data_away_q1_point(df=df)\n",
        "    X_train,X_test,Y_train,Y_test = data_split_regresyon(X,Y)\n",
        "    model_ , score__ = build_regressor_model(X_train,Y_train,X_test,Y_test,name=\"model/teams/away_q1_point_model.pkl\")\n",
        "    return model_,X_train,X_test,Y_train,Y_test\n",
        "def build_away_q2_point_predict_model(csv_url=\"../input/matches_w_player_stats.csv\"):\n",
        "    df,Y=preprocess_away_q2_point(csv_url)\n",
        "    X,_ = split_data_away_q2_point(df=df)\n",
        "    X_train,X_test,Y_train,Y_test = data_split_regresyon(X,Y)\n",
        "    model_ , score__ = build_regressor_model(X_train,Y_train,X_test,Y_test,name=\"model/teams/away_q2_point_model.pkl\")\n",
        "    return model_,X_train,X_test,Y_train,Y_test\n",
        "def build_away_q3_point_predict_model(csv_url=\"../input/matches_w_player_stats.csv\"):\n",
        "    df,Y=preprocess_away_q3_point(csv_url)\n",
        "    X,_ = split_data_away_q3_point(df=df)\n",
        "    X_train,X_test,Y_train,Y_test = data_split_regresyon(X,Y)\n",
        "    model_ , score__ = build_regressor_model(X_train,Y_train,X_test,Y_test,name=\"model/teams/away_q3_point_model.pkl\")\n",
        "    return model_,X_train,X_test,Y_train,Y_test\n",
        "def build_away_q4_point_predict_model(csv_url=\"../input/matches_w_player_stats.csv\"):\n",
        "    df,Y=preprocess_away_q4_point(csv_url)\n",
        "    X,_ = split_data_away_q4_point(df=df)\n",
        "    X_train,X_test,Y_train,Y_test = data_split_regresyon(X,Y)\n",
        "    model_ , score__ = build_regressor_model(X_train,Y_train,X_test,Y_test,name=\"model/teams/away_q4_point_model.pkl\")\n",
        "    return model_,X_train,X_test,Y_train,Y_test"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mfiKYMH4nZno"
      },
      "source": [
        "# 9-Getting Prediction with easier way"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "aIiTM12vnZno"
      },
      "source": [
        "def get_result(X, model):\n",
        "    return model.predict_proba(X)\n",
        "def get_point_result(X, model):\n",
        "    return model.predict(X)\n",
        "def get_points(X,model):\n",
        "    return model.predict(X) ,model.predict_proba(X)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gdstrmuynZno"
      },
      "source": [
        "# 10-Train_Test_Split"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "NxsQKFplnZnp"
      },
      "source": [
        "def data_split(X,Y,test_size=0.1):\n",
        "    X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=test_size, random_state=42)\n",
        "    Y_train = Y_train.reshape(-1, 1)\n",
        "    Y_test = Y_test.reshape(-1, 1)\n",
        "    print(X_train.shape, X_test.shape, Y_train.shape, Y_test.shape)\n",
        "    return X_train,X_test,Y_train,Y_test\n",
        "def data_split_regresyon(X,Y,test_size=0.1):\n",
        "    X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=test_size, random_state=42)\n",
        "    Y_train = Y_train.reshape(-1, 1)\n",
        "    Y_test = Y_test.reshape(-1, 1)\n",
        "    print(X_train.shape, X_test.shape, Y_train.shape, Y_test.shape)\n",
        "    return X_train,X_test,Y_train,Y_test"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "maWIM4kZnZnp"
      },
      "source": [
        "# 11-Easier Prediction Functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "eERLdmpVnZnp"
      },
      "source": [
        "def predict_match_result(data):\n",
        "    model = pickle.load(open(\"model/xgb_classifier.pkl\",\"rb\"))\n",
        "    encoded= label_test_data(data)\n",
        "    scalered = standart_scaler_test_data(encoded)\n",
        "    df = onehotencoder_test_data(scalered)\n",
        "    print(df.head())\n",
        "    X, Y = split_data_by_result(df=df,result_index=60)\n",
        "    return get_result(X, model),int(Y[0])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "vaaRfwSznZnp"
      },
      "source": [
        "def predict_home_point(data):\n",
        "    model = pickle.load(open(\"model/teams/home_point_model.pkl\",\"rb\"))\n",
        "    Y = np.array(data.teamPTS)\n",
        "    encoded= label_test_data(data)\n",
        "    scalered = standart_scaler_test_data(encoded)\n",
        "    df = onehotencoder_test_data(scalered)\n",
        "    X, _ = split_data_home_point(df=df)\n",
        "    return get_point_result(X, model),int(Y[0])\n",
        "def predict_away_point(data):\n",
        "    model = pickle.load(open(\"model/teams/away_point_model.pkl\",\"rb\"))\n",
        "    Y = np.array(data.opptPTS)\n",
        "    encoded= label_test_data(data)\n",
        "    scalered = standart_scaler_test_data(encoded)\n",
        "    df = onehotencoder_test_data(scalered)\n",
        "    X, _ = split_data_away_point(df=df)\n",
        "    return get_point_result(X, model),int(Y[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "DDVgewu5nZnq"
      },
      "source": [
        "def predict_home_q1_point(data):\n",
        "    model = pickle.load(open(\"model/teams/home_q1_point_model.pkl\",\"rb\"))\n",
        "    Y = np.array(data.teamPTS1)\n",
        "    encoded= label_test_data(data)\n",
        "    scalered = standart_scaler_test_data(encoded)\n",
        "    df = onehotencoder_test_data(scalered)\n",
        "    X, _ = split_data_home_q1_point(df=df)\n",
        "    return get_point_result(X, model),int(Y[0])\n",
        "def predict_home_q2_point(data):\n",
        "    model = pickle.load(open(\"model/teams/home_q2_point_model.pkl\",\"rb\"))\n",
        "    Y = np.array(data.teamPTS2)\n",
        "    encoded= label_test_data(data)\n",
        "    scalered = standart_scaler_test_data(encoded)\n",
        "    df = onehotencoder_test_data(scalered)\n",
        "    X, _ = split_data_home_q2_point(df=df)\n",
        "    return get_point_result(X, model),int(Y[0])\n",
        "def predict_home_q3_point(data):\n",
        "    model = pickle.load(open(\"model/teams/home_q3_point_model.pkl\",\"rb\"))\n",
        "    Y = np.array(data.teamPTS3)\n",
        "    encoded= label_test_data(data)\n",
        "    scalered = standart_scaler_test_data(encoded)\n",
        "    df = onehotencoder_test_data(scalered)\n",
        "    X, _ = split_data_home_q3_point(df=df)\n",
        "    return get_point_result(X, model),int(Y[0])\n",
        "def predict_home_q4_point(data):\n",
        "    model = pickle.load(open(\"model/teams/home_q4_point_model.pkl\",\"rb\"))\n",
        "    Y = np.array(data.teamPTS4)\n",
        "    encoded= label_test_data(data)\n",
        "    scalered = standart_scaler_test_data(encoded)\n",
        "    df = onehotencoder_test_data(scalered)\n",
        "    X, _ = split_data_home_q4_point(df=df)\n",
        "    return get_point_result(X, model),int(Y[0])\n",
        "def predict_away_q1_point(data):\n",
        "    model = pickle.load(open(\"model/teams/away_q1_point_model.pkl\",\"rb\"))\n",
        "    Y = np.array(data.opptPTS1)\n",
        "    encoded= label_test_data(data)\n",
        "    scalered = standart_scaler_test_data(encoded)\n",
        "    df = onehotencoder_test_data(scalered)\n",
        "    X, _ = split_data_away_q1_point(df=df)\n",
        "    return get_point_result(X, model),int(Y[0])\n",
        "def predict_away_q2_point(data):\n",
        "    model = pickle.load(open(\"model/teams/away_q2_point_model.pkl\",\"rb\"))\n",
        "    Y = np.array(data.opptPTS2)\n",
        "    encoded= label_test_data(data)\n",
        "    scalered = standart_scaler_test_data(encoded)\n",
        "    df = onehotencoder_test_data(scalered)\n",
        "    X, _ = split_data_away_q2_point(df=df)\n",
        "    return get_point_result(X, model),int(Y[0])\n",
        "def predict_away_q3_point(data):\n",
        "    model = pickle.load(open(\"model/teams/away_q3_point_model.pkl\",\"rb\"))\n",
        "    Y = np.array(data.opptPTS3)\n",
        "    encoded= label_test_data(data)\n",
        "    scalered = standart_scaler_test_data(encoded)\n",
        "    df = onehotencoder_test_data(scalered)\n",
        "    X, _ = split_data_away_q3_point(df=df)\n",
        "    return get_point_result(X, model),int(Y[0])\n",
        "def predict_away_q4_point(data):\n",
        "    model = pickle.load(open(\"model/teams/away_q4_point_model.pkl\",\"rb\"))\n",
        "    Y = np.array(data.opptPTS4)\n",
        "    encoded= label_test_data(data)\n",
        "    scalered = standart_scaler_test_data(encoded)\n",
        "    df = onehotencoder_test_data(scalered)\n",
        "    X, _ = split_data_away_q4_point(df=df)\n",
        "    return get_point_result(X, model),int(Y[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2EI2anKpnZnq"
      },
      "source": [
        "# 12-You can use build_all_in_one to build all models one line code."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "QpkpBfifnZnq"
      },
      "source": [
        "def build_all_in_one(csv_url=\"/content/inputs/matches_w_player_stats.csv\",test_csv=\"test_data/GSWvsHOU_test_data.csv\"):\n",
        "\n",
        "\n",
        "    model_, X_train, X_test, Y_train, Y_test = build_class_model(csv_url=csv_url)\n",
        "\n",
        "    model_, X_train, X_test, Y_train, Y_test = build_home_q1_point_predict_model(csv_url=csv_url)\n",
        "    model_, X_train, X_test, Y_train, Y_test = build_home_q2_point_predict_model(csv_url=csv_url)\n",
        "    model_, X_train, X_test, Y_train, Y_test = build_home_q3_point_predict_model(csv_url=csv_url)\n",
        "    model_, X_train, X_test, Y_train, Y_test = build_home_q4_point_predict_model(csv_url=csv_url)\n",
        "    model_, X_train, X_test, Y_train, Y_test = build_away_q1_point_predict_model(csv_url=csv_url)\n",
        "    model_, X_train, X_test, Y_train, Y_test = build_away_q2_point_predict_model(csv_url=csv_url)\n",
        "    model_, X_train, X_test, Y_train, Y_test = build_away_q3_point_predict_model(csv_url=csv_url)\n",
        "    model_, X_train, X_test, Y_train, Y_test = build_away_q4_point_predict_model(csv_url=csv_url)\n",
        "    model_, X_train, X_test, Y_train, Y_test = build_home_point_predict_model(csv_url=csv_url)\n",
        "    model_, X_train, X_test, Y_train, Y_test = build_away_point_predict_model(csv_url=csv_url)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 425
        },
        "id": "gfGSx2JMnZnq",
        "outputId": "ffe44c78-982f-4fc2-aa38-787701675b10"
      },
      "source": [
        " list_ = get_test_data(teamAbbr=\"LAC\",opptAbbr= \"POR\", homeplayers=[\"Caron Butler\", \"Blake Griffin\", \"DeAndre Jordan\", \"Randy Foye\",\n",
        "                         \"Chris Paul\", \"Mo Williams\", \"Brian Cook\", \"Ryan Gomes\", \"Trey Thompkins\", \"Chauncey Billups\",\n",
        "                         \"Eric Bledsoe\"],awayplayers=[\"Caron Butler\", \"Blake Griffin\", \"DeAndre Jordan\", \"Randy Foye\", \"Chris Paul\", \"Mo Williams\",\n",
        "                         \"Brian Cook\", \"Ryan Gomes\", \"Trey Thompkins\", \"Chauncey Billups\", \"Eric Bledsoe\"],\n",
        "                          last_x_match=10)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-230-eef8ea67f207>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m                         \u001b[0;34m\"Eric Bledsoe\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mawayplayers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"Caron Butler\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Blake Griffin\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"DeAndre Jordan\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Randy Foye\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Chris Paul\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Mo Williams\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m                         \"Brian Cook\", \"Ryan Gomes\", \"Trey Thompkins\", \"Chauncey Billups\", \"Eric Bledsoe\"],\n\u001b[0;32m----> 5\u001b[0;31m                          last_x_match=10)\n\u001b[0m",
            "\u001b[0;32m<ipython-input-199-bee350a0cab3>\u001b[0m in \u001b[0;36mget_test_data\u001b[0;34m(teamAbbr, opptAbbr, homeplayers, awayplayers, data_csv_name, player_stats_csv_name, last_x_match)\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mremove_whitespaces_in_df_columns\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# clean column names\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_column_names\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# get column names and assign\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0mall_players_stats_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mplayer_stats_csv_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mindex_col\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m     \u001b[0mplay_by_play\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_play_by_play_stats\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlast_x_match\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0mtest_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mplay_by_play\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mplay_by_play\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"teamAbbr\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mteamAbbr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m&\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mplay_by_play\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"opptAbbr\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mopptAbbr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    686\u001b[0m     )\n\u001b[1;32m    687\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 688\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    689\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    690\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    452\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    453\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 454\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfp_or_buf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    455\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    456\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    946\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    947\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 948\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    949\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    950\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1178\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"c\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1179\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"c\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1180\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1181\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1182\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"python\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   2008\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"usecols\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2009\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2010\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2011\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2012\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../input/all_player_stats.csv'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "MFN_BUnInZnr"
      },
      "source": [
        "os.listdir(\"test_data\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "A_jKOPsdnZnr"
      },
      "source": [
        "build_all_in_one(test_csv=\"test_data/LACvsPOR_test_data.csv\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8n_hojBanZnr"
      },
      "source": [
        "# 13-Test our models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "GppBlBwAnZnr"
      },
      "source": [
        "df = pd.read_csv(\"test_data/LACvsPOR_test_data.csv\", index_col=0)\n",
        "df.drop(\"index\", axis=1, inplace=True)\n",
        "df.fillna(0, inplace=True)\n",
        "df_2 = df.copy()\n",
        "df_3 = df.copy()\n",
        "df_4 = df.copy()\n",
        "df_5 = df.copy()\n",
        "df_6 = df.copy()\n",
        "df_7 = df.copy()\n",
        "df_8 = df.copy()\n",
        "df_9 = df.copy()\n",
        "df_10 = df.copy()\n",
        "df_11 = df.copy()\n",
        "print(predict_match_result(df_11))\n",
        "print(predict_home_q1_point(df_5))\n",
        "print(predict_home_q2_point(df_6))\n",
        "print(predict_home_q3_point(df_7))\n",
        "print(predict_home_q4_point(df_8))\n",
        "print(predict_away_q1_point(df))\n",
        "print(predict_away_q2_point(df_2))\n",
        "print(predict_away_q3_point(df_3))\n",
        "print(predict_away_q4_point(df_4))\n",
        "print(predict_home_point(df_9)) \n",
        "print(predict_away_point(df_10))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "H2wkIGkXnZnr"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}